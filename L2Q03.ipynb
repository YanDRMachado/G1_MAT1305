{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n# import statistics\n# import math\n# import pandas as pd\n# from autograd import grad\n\ndef pontos_2d(w,P): #função para gerar os pontos aleatórios\n    X = [np.ones(P), 2*np.random.random_sample((P,))-1, 2*np.random.random_sample((P,))-1]\n    # X = [np.ones(P),np.random.random_sample(P,),np.random.random_sample((P,))]\n    X = np.array(X)\n    Y = []\n    for i in range(P): \n        if X[2][i]**2 * w[2] + X[1][i]**2 * w[1] + X[0][i]*w[0] < 0:\n            Y.append(-1)\n        else:\n            Y.append(1)\n    return X, Y",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def f(x):\n    return x**2\n\ndef modelx(w,x):\n    return np.dot(f(x),w)\n\ndef model(w,x):\n    return x[2]**2 * w[2] + x[1]**2 * w[1] + x[0]*w[0]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def g_lr_softmax(w,X,Y):\n    # N = len(X[:,0])\n    P = len(X[0,:])\n    cost = 0\n    for p in range(P):\n        c = model(w,X[:,p])\n        # print([c,Y[p]])\n        cost = cost + np.log((1+np.exp(-Y[p]*c)))\n        # print(cost)\n    cost = cost/P\n    return cost\n\ndef grad_lr_softmax(w,X,Y):\n    N = len(X[:,0])\n    P = len(X[0,:])\n    grad = np.zeros(N)\n    for p in range(P):\n        c = model(w,X[:,p])\n        k = Y[p] * np.exp(-Y[p]*c) / (1 + np.exp(-Y[p]*c))\n        grad = grad + k * f(X[:,p])\n    grad = -grad/P\n    return grad",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def gradient_descent(alpha, max_its, w, X,Y):\n    weight_history = [w]\n    cost_history = [g_lr_softmax(w,X,Y)]\n    tol = alpha\n    for k in range(max_its):\n        grad_eval = grad_lr_softmax(w,X,Y)\n        # print('grad = ', grad_eval)\n        \n        w = w - alpha*grad_eval\n        # print('w = ',w)\n        # print('custo = ', g_lr_softmax(w,X,Y))\n        \n        weight_history.append(w)\n        cost_history.append(g_lr_softmax(w,X,Y))\n        norm = np.linalg.norm(grad_eval)\n        if norm < tol:\n            print('alpha ', alpha)\n            alpha = alpha/10\n            tol = tol/10\n    return weight_history, cost_history",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "P = 500\nw = np.array([1,-1/0.64,-1/0.25])\n[X,Y] = pontos_2d(w,P)\nN = 2\nmax_its = 500\nalpha = 0.01\n[w,cost] = gradient_descent(alpha,max_its,w,X,Y)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#mudando as características do plot para o espaço de features conforme figura da lista\n\nfor k in range(1,max_its,500):\n    plt.figure(figsize = (8,8))\n    for i in range(P):\n        if Y[i] > 0:\n            plt.scatter(X[1,i]**2,X[2,i]**2,c = 'r')\n        else:\n            plt.scatter(X[1,i]**2,X[2,i]**2,c = 'b')\n    x = np.linspace(0,3/5,P)\n    y = -(w[k-1][1] * x + w[k-1][0]) / w[k-1][2]\n    plt.plot(x, y, c = 'g')\n    plt.show()\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}