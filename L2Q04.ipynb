{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n# import statistics\n# import math\n# import pandas as pd\n# from autograd import grad\n\ndef pontos_2d(w,P): #função para gerar os pontos aleatórios\n    X = [np.ones(P), 2*np.random.random_sample((P,))-1, 2*np.random.random_sample((P,))-1]\n    # X = [np.ones(P),np.random.random_sample(P,),np.random.random_sample((P,))]\n    X = np.array(X)\n    Y = []\n    for i in range(P): \n        if X[2][i]**2 * w[2] + X[1][i]**2 * w[1] + X[0][i]*w[0] < 0:\n            Y.append(-1)\n        else:\n            Y.append(1)\n    return X,Y",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def f(x):\n    return x**2\n\ndef modelx(w,x):\n    return np.dot(f(x),w)\n\ndef model(w,x):\n    return x[2]**2 * w[2] + x[1]**2 * w[1] + x[0]*w[0]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def g_lr_softmax(w,X,Y):\n    # N = len(X[:,0])\n    P = len(X[0,:])\n    cost = 0\n    for p in range(P):\n        c = model(w,X[:,p])\n        # print([c,Y[p]])\n        cost = cost + np.log((1+np.exp(-Y[p]*c)))\n        # print(cost)\n    cost = cost/P\n    return cost\n\ndef grad_lr_softmax(w,X,Y):\n    N = len(X[:,0])\n    P = len(X[0,:])\n    grad = np.zeros(N)\n    for p in range(P):\n        c = model(w,X[:,p])\n        k = Y[p] * np.exp(-Y[p]*c) / (1 + np.exp(-Y[p]*c))\n        grad = grad + k * f(X[:,p])\n    grad = -grad/P\n    return grad",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}