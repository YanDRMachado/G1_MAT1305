{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport statistics #usei para o cálculo da média e desvio padrão\n# import math\n# import pandas as pd\n# from autograd import grad\n\ndef pontos_2d(P): #função para gerar os pontos aleatórios\n    X = 5*np.random.random_sample(P,)\n    X = np.array(X)\n    Y = np.sin(X[:]) + 0.2 * np.random.random_sample(P,)\n    meanX = statistics.mean(X)\n    stdevX = statistics.stdev(X)\n    return X, Y, meanX, stdevX #serão usados para a normalização conforme pedido",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "P = 100\n[X,Y,mean,stdev] = pontos_2d(P)\nprint('mean, stdev: ', mean, ',', stdev)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#---------------------------------não normalizado-----------------------------\n\ndef f(v,x):\n    return np.sin(v[0] + x * v[1])\n\ndef model(w,v,x_p):\n    a = w[0] + f(v,x_p) * w[1]\n    return a\n    \ndef NL_regression(w,v,x,y):\n    P = len(x)\n    cost = 0\n    for p in range(P):\n        cost = cost + (model(w,v,x[p]) - y[p])**2\n    cost /= P\n    return cost\n\ndef grad_NL_regression(w,v,x,y):\n    P = len(x)\n    grad = np.zeros(4)\n    for p in range(P):\n        k = 2*(model(w,v,x[p]) - y[p])\n        grad[0] += k\n        grad[1] += k * f(v,x[p])\n        grad[2] += k * np.cos(v[0] + x[p] * v[1])*w[1]\n        grad[3] += k * np.cos(v[0] + x[p] * v[1])*w[1]*x[p]\n    grad /= P\n    return grad\n\ndef gradient_descent(x,y,alpha,max_its):\n    w_theta = [1,1,1,1] #valores iniciais\n    w_theta = np.array(w_theta)\n    weight_history = [w_theta]\n    w = np.zeros(2)\n    v = np.zeros(2)\n    w[0] = w_theta[0]\n    w[1] = w_theta[1]\n    v[0] = w_theta[2]\n    v[1] = w_theta[3]\n    cost_history = [NL_regression(w,v,x,y)]\n    tol = 0.1\n    for k in range(max_its):\n        w_theta = w_theta - alpha * grad_NL_regression(w,v,x,y)\n        weight_history.append(w_theta)\n        cost_history.append(NL_regression(w,v,x,y))\n        g = grad_NL_regression(w,v,x,y)\n        norm = np.linalg.norm(g)\n        if norm < tol:\n            alpha = alpha/10\n            tol = tol/10\n        w[0] = w_theta[0]\n        w[1] = w_theta[1]\n        v[0] = w_theta[2]\n        v[1] = w_theta[3]\n    return weight_history, cost_history\n\n\nmax_its = 500\nalpha = 0.01\n[w_h, cost] = gradient_descent(X,Y,alpha,max_its)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#---------------------------------normalizado----------------------------------\n\ndef f_norm(v,x):\n    return f(v,x) - mean / stdev\n    \ndef model_norm(w,v,x_p):\n    a = w[0] + (f_norm(v,x_p) * w[1])\n    return a\n    \ndef NL_regression_norm(w,v,x,y):\n    P = len(x)\n    cost = 0\n    for p in range(P):\n        cost = cost + (model_norm(w,v,x[p]) - y[p])**2\n    cost /= P\n    return cost\n\ndef grad_NL_regression_norm(w,v,x,y):\n    P = len(x)\n    grad = np.zeros(4)\n    for p in range(P):\n        k = 2*(model_norm(w,v,x[p]) - y[p])\n        grad[0] += k\n        grad[1] += k * f_norm(v,x[p])\n        \n        # recomendação da correção\n        grad[2] += k * np.cos(v[0] + (x[p] - mean / stdev) * v[1])*w[1]\n        grad[3] += k * np.cos(v[0] + (x[p] - mean / stdev) * v[1])*w[1]*x[p]\n        \n        # grad[2] += k * np.cos(v[0] + x[p] * v[1])*w[1]\n        # grad[3] += k * np.cos(v[0] + x[p] * v[1])*w[1]*x[p]\n    grad /= P\n    return grad\n\ndef gradient_descent_norm(x,y,alpha,max_its):\n    w_theta = [1,1,1,1] #valores iniciais\n    w_theta = np.array(w_theta)\n    weight_history = [w_theta]\n    w = np.zeros(2)\n    v = np.zeros(2)\n    w[0] = w_theta[0]\n    w[1] = w_theta[1]\n    v[0] = w_theta[2]\n    v[1] = w_theta[3]\n    cost_history = [NL_regression_norm(w,v,x,y)]\n    tol = 0.1\n    for k in range(max_its):\n        w_theta = w_theta - alpha * grad_NL_regression_norm(w,v,x,y)\n        weight_history.append(w_theta)\n        cost_history.append(NL_regression_norm(w,v,x,y))\n        g = grad_NL_regression(w,v,x,y)\n        norm = np.linalg.norm(g)\n        if norm < tol:\n            alpha = alpha/10\n            tol = tol/10\n        w[0] = w_theta[0]\n        w[1] = w_theta[1]\n        v[0] = w_theta[2]\n        v[1] = w_theta[3]\n    return weight_history, cost_history\n\n[w_h_norm, cost_norm] = gradient_descent_norm(X,Y,alpha,max_its)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#-------------plot para comparar normalizado x sem normalizar (custo x steps)\n\nit = list(range(0,max_its+1)) #gerando 500 pontos para o eixo X do gráfico\nplt.figure(figsize=(14,8), dpi=80)\nplt.scatter(it, cost, marker='o', color='b', label='Cost')\nplt.scatter(it, cost_norm, marker='.', color='r', label='Normalized Cost')\nplt.title(\"Cost Function x Iterations\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Cost Value\")\nplt.legend()\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}