{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef pontos_2d(w,P): \n    X = [np.ones(P),np.random.random_sample(P,), np.random.random_sample((P,))]\n    X = np.array(X)\n    Y = []\n    for i in range(P):\n        if X[2][i]*w[2] + X[1][i]*w[1] + X[0][i]*w[0] < 0:\n            Y.append(1)\n            X[2][i] += 0.1\n        else:\n            Y.append(-1)\n            X[2][i] -= 0.05\n    return X,Y",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def g_lr_softmax(w,x,y,lamb): #aqui lamb Ã© a constante que multiplica o termo \"regularizer\" (6.47)\n    P = len(x[0,:])\n    N = len(x[:,0])\n    cost = 0\n    for p in range(P):\n        c = np.dot(x[:,p], w)\n        cost = cost + np.log(1 + np.exp(-y[p]*c) + lamb * np.linalg.norm(w)**2) \n    cost = cost/P\n    return cost",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def grad_lr_softmax(w,x,y):\n    P = len(x[0,:])\n    N = len(x[:,0])\n    grad = np.zeros(N)\n    for p in range(P):\n        c = np.dot(x[:,p], w)\n        k = y[p] * np.exp(-y[p]*c) / (1+np.exp(-y[p]*c))\n        grad = grad + k * x[:,p] + lamb*w*2\n    grad = -grad/P\n    return grad   ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def gradient_descent(w,x,y,alpha,max_its):\n    w_history = [w]\n    cost_history = [g_lr_softmax(w,x,y,lamb)]\n    for k in range(max_its):\n        w = w - alpha * grad_lr_softmax(w, x, y)\n        w_history.append(w)\n        cost_history.append(g_lr_softmax(w, x, y,lamb))\n        #print('cost = ',g_lr_softmax(w,x,y))\n        #print('w', w)\n        #print('grad',grad_lr_softmax(w,x,y) )\n    return w_history, cost_history",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "P = 100\nw = np.array([-0.3, -0.8, 1])\n[X,Y] = pontos_2d(w,P)\n# print(X)\n# print(Y)\nN = 3\nW = np.ones(N)\nmax_its = 500\nlamb = 0.001 #utilizando lambda = 0.001 (10^-3) conforme solicitado no exemplo 6.7 do livro\nalpha = 1\n\n[w,cost] = gradient_descent(w, X, Y, alpha, max_its)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for k in range(1, max_its, 50):\n    plt.figure(figsize = (8,8))\n    for i in range(P):\n        if Y[i] > 0:\n            plt.scatter(X[1,i],X[2,i],c='b')\n        else:\n            plt.scatter(X[1,i],X[2,i],c='r')\n    x = np.linspace(0,1,100)\n    y = - (w[k-1][1]*x+w[k-1][0])/w[k-1][2]\n    plt.plot(x,y,'g')\n    plt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}